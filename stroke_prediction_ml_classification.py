# -*- coding: utf-8 -*-
"""Stroke_prediction_ML_Classification.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1t9V2MB6W9djSRTMoLP6FPx-V8sgiCGod

#              **Arjumand Afreen Tabinda**
#              **411210**

---

# 🧠 **Comprehensive Insight into Stroke Prediction Dataset**

---

## 📊 **Data Summary**

**Total Entries:** 5110
**Total Attributes:** 12

---

## 🧾 **Feature Breakdown**


| 🧩 Column             | 📖 Description                                                       |
| --------------------- | -------------------------------------------------------------------- |
| **id**                | Unique patient identifier                                            |
| **gender**            | Patient’s gender (Male / Female / Other)                             |
| **age**               | Age of the patient (in years)                                        |
| **hypertension**      | 1 → Has hypertension, 0 → No hypertension                            |
| **heart_disease**     | 1 → Suffers from heart disease, 0 → No heart issue                   |
| **ever_married**      | Indicates if the patient was ever married (Yes/No)                   |
| **work_type**         | Type of occupation (Private, Self-employed, Govt_job, etc.)          |
| **Residence_type**    | Area of residence (Urban / Rural)                                    |
| **avg_glucose_level** | Average glucose concentration in the blood                           |
| **bmi**               | Body Mass Index (contains some missing values)                       |
| **smoking_status**    | Smoking behavior (never smoked / smokes / formerly smoked / Unknown) |
| **stroke**            | 🎯 Target variable — 1 if stroke occurred, 0 otherwise               |

---

## 🎯 **Core Objective**


The aim of this dataset is to **predict the likelihood of stroke** in a patient using various **health, lifestyle, and demographic indicators**.
This task is a **binary classification problem**, where the model forecasts:

* **1 → Stroke likely**
* **0 → No stroke**

---

## 🧠 **Why This Dataset Matters**


* 🩺 **Stroke** ranks among the top global health threats.
* ⚕ **Early prediction** empowers proactive medical care and prevention.
* 🔬 **Rich feature diversity** (medical, demographic, lifestyle) makes it perfect for exploring relationships and building predictive intelligence.

---

## 🎯 **Target Attribute Details**


| Feature    | Type           | Purpose                                                 |
| ---------- | -------------- | ------------------------------------------------------- |
| **stroke** | Binary (0 / 1) | Determines whether the patient has experienced a stroke |

---

# **Step 1: Importing Required Libraries**
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

"""---

### 🐼 **pandas**
**Role**: Toolkit for organizing and exploring data.

**Why it matters**:
- Reads files like CSVs with ease.
- Offers smart containers like `DataFrame` and `Series` for table-style data.
- Simplifies cleaning, slicing, and summarizing information.

**Example**:  
`pd.read_csv('data.csv')` → imports your dataset for analysis.

---

### 📊 **matplotlib.pyplot**
**Role**: Charting library for static and dynamic visuals.

**Why it matters**:
- Builds bar graphs, line plots, histograms, and scatter diagrams.
- Reveals trends, patterns, and anomalies through visuals.

**Example**:  
`plt.plot(x, y)` or `plt.hist(data)` → shows data shapes and distributions.

---

### 🌈 **seaborn**
**Role**: Stylish plotting layer built on Matplotlib.

**Why it matters**:
- Creates elegant visuals like heatmaps, boxplots, and pairplots.
- Makes statistical graphics more intuitive and beautiful.

**Example**:  
`sns.heatmap(df.corr())` → shows feature relationships via color.

---

### 🔢 **numpy**
**Role**: Math engine for arrays and number crunching.

**Why it matters**:
- Performs fast calculations on large datasets.
- Powers preprocessing, stats, and supports pandas + ML tools.

**Example**:  
`np.mean(df['age'])` → calculates average age from your data.

---

# **📂 Step 2: Loading the Dataset**
"""

import pandas as pd

df = pd.read_csv('/content/stroke.csv')
df.head()

"""---

### 🎯 **Purpose**

This step is used to **load the dataset** into a pandas **DataFrame** so that it can be easily explored and analyzed.

---

### 🧠 **Explanation**

`pd.read_csv('/content/stroke.csv')` → Reads the CSV (Comma Separated Values) file named **stroke.csv** and loads it into a **pandas DataFrame**.
A **DataFrame** is a two-dimensional table with labeled **rows and columns**, which makes it ideal for data handling and analysis.

Typing `df` → Displays the first few rows of the dataset (by default) in Jupyter or Colab, giving a quick view of the data’s structure and contents.

---

### 📊 **Output**

The output shows a **table-like structure** containing columns such as *id, gender, age, hypertension, heart_disease,* etc.
This helps you get a **first impression** of what type of data you are working with and how it looks.

#🧾 **Step 3: Previewing the Top Entries**
"""

df.head()

"""#📄 **Step 4: Looking at the Last Records**"""

df.tail()

"""---

### 🌟 **Objective**

The `df.tail()` function is used to **view the last few rows** of a dataset. It allows you to check the **ending section** of the data to confirm that it’s complete and doesn’t have any **missing, repeated, or inconsistent entries** near the bottom.

---

### 🧩 **Description**

`df.tail()` → By default, it shows the **last 5 rows** of the DataFrame.
👉 It’s helpful for spotting issues like **incomplete data**, **extra headers**, or **blank rows** at the dataset’s end.
You can also specify the number of rows to display:

```python
df.tail(10)
```

This will show the **last 10 rows** instead of the default 5.

---

### 📈 **Result**

The output presents the **final few rows** of the dataset, including familiar columns such as *id, gender, age, bmi, avg_glucose_level,* and *stroke*, helping you ensure that your data ends correctly.

# **🧩 Step 5: Checking Dataset Information**
"""

df.info()

"""🎯 **Purpose**
The `df.info()` function gives a quick summary of your dataset. It helps you understand the overall structure and data types of every column in your DataFrame.

💡 **Explanation**
When you use `df.info()`, it shows:

* Total number of **rows and columns** in the dataset.
* **Column names** (the dataset’s features).
* **Non-null count** → how many values are **not missing** in each column.
* **Data types (dtype)** of each column — for example, `int64`, `float64`, or `object`.
* **Memory usage** → how much space the DataFrame uses in memory.

🧩 This function helps you quickly understand your dataset’s **structure**, detect **missing values**, and identify **variable types** before starting preprocessing or model building.

# **🔍 Step 6: Checking Data Types of Each Column**
"""

df.dtypes

"""🎯 **Purpose**
The `df.dtypes` command is used to **identify the data type** of every column in your dataset. It helps you recognize whether a feature is **numerical, categorical, or textual**, which is essential before performing preprocessing or training a model.

💡 **Explanation**
Each column in a DataFrame has a particular **data type (dtype)** that defines what kind of values it holds:

* **int64** → Whole numbers (e.g., age, count)
* **float64** → Decimal numbers (e.g., BMI, glucose level)
* **object** → Text or category values (e.g., gender, work type)
* **bool** → True/False values
* **datetime64** → Date or time information

🧩 Understanding data types helps you decide:

* Which columns require **encoding** (for categorical data)
* Which ones need **scaling or normalization** (for numerical data)
* Where **type conversion** might be necessary before model training

# **📊 Step 7: Descriptive Statistics of the Dataset**
"""

df.describe()

"""✨ **Objective**

The `df.describe()` function gives a **statistical overview** of all the numerical columns in your dataset. It helps you quickly understand the **data distribution**, **central values**, and **variation** within your data.

💬 **Details**
This function automatically computes and shows key **statistical indicators** for numeric columns, including:

| Metric       | Description                                             |
| ------------ | ------------------------------------------------------- |
| **count**    | Total number of non-missing entries                     |
| **mean**     | Average value of the column                             |
| **std**      | Standard deviation — measures how much the data varies  |
| **min**      | Smallest value in the column                            |
| **25% (Q1)** | First quartile — 25% of the data falls below this point |
| **50% (Q2)** | Median — the central value of the data                  |
| **75% (Q3)** | Third quartile — 75% of the data falls below this point |
| **max**      | Largest value in the column                             |

🪄 It’s a quick and useful way to explore your data’s **range**, **consistency**, and **distribution pattern** before preprocessing or modeling.

# **🧾 Step 8: Displaying All Column Names**
"""

df.columns

"""🚀 **Aim**


The `df.columns` attribute is used to **list all the column names** in your dataset. It helps you quickly see the **features (independent variables)** and the **target column** available for analysis or model building.

💡 **Insight**
Each dataset includes multiple **columns (features)** that describe different characteristics of the data.

When you use `df.columns`, it returns an **Index object** showing all the column names in the DataFrame.

It’s helpful for:

* Spotting **spelling mistakes or extra spaces** in column names.
* **Renaming** columns where needed.
* Getting a clear view of your dataset’s **structure** before moving ahead with preprocessing or modeling.

# **📐 Step 9: Checking the Shape of the Dataset**
"""

df.shape

"""📏 **Goal**
The `df.shape` attribute is used to **check the size of your dataset**, showing how many **rows** and **columns** it has. It gives a quick idea of your DataFrame’s overall dimensions.

🔎 **Overview**
`df.shape` returns a **tuple (rows, columns)** where:

* The **first number** shows total **rows** (data records).
* The **second number** shows total **columns** (features).

It’s useful for understanding:

* The **scale** of your dataset.
* Whether it’s **large enough** for analysis or model training.
* If the **data loaded correctly** without missing rows or import issues.

#🪄 **Step 10: Identifying Repeated Entries in the Dataset**
"""

df.duplicated().sum()

"""💫 **Objective**
The `df.duplicated().sum()` command is used to **detect and count** any repeated rows in the dataset.

Duplicate entries can introduce **bias**, lead to **incorrect analysis**, and produce **unreliable model results**, so finding and managing them is a key part of **data cleaning**.

💡 **Details**

* `df.duplicated()` returns a **Boolean Series** where:

  * **True** → the row is a duplicate of an earlier one.
  * **False** → the row is unique.
* Using **.sum()** adds up all the `True` values, showing the **total number of duplicate rows** in your dataset.

#🔍 **Step 11: Detecting Null or Empty Values in the Dataset**
"""

df.isnull().sum()

"""🌸 **Objective**
The `df.isnull().sum()` command helps to **find and count** all the missing (NaN or null) values in every column of the dataset.

Recognizing missing information is a **crucial part of data cleaning**, as it can strongly impact the **accuracy and reliability** of your model.

💫 **Explanation**

* `df.isnull()` creates a **Boolean DataFrame** where:

  * **True** → shows that a value is missing.
  * **False** → shows that a value is available.
* When you apply **.sum()**, it counts the total number of **True** values — meaning the **total missing entries** in each column.
.

# **🧮 Step 12: Handling Missing Values in the Dataset**
"""

df['bmi'].fillna(df['bmi'].mean(), inplace=True)

df.isnull().sum()

"""# 🌟 **Purpose**

This command is used to **fill the missing values** in the `bmi` column with the **mean (average)** value of that same column.

It helps make sure there are **no empty or null values left**, keeping the dataset **clean and ready** for analysis or model training.

---

# 💡 **Explanation**

Let’s understand each part of the command 👇

| 🧩 **Part**        | 💬 **Meaning**                                           |
| ------------------ | -------------------------------------------------------- |
| `df['bmi']`        | Selects the **bmi** column from the dataset              |
| `.fillna()`        | Function that **replaces missing (NaN)** values          |
| `df['bmi'].mean()` | Calculates the **average** of all existing BMI values    |
| `inplace=True`     | **Directly updates** the DataFrame without making a copy |

✨ **In short:**
This command **finds the mean BMI value** and **fills all missing entries (NaN)** in the `bmi` column with that average — ensuring the dataset is **complete and consistent**.

# **🎯  Step 13: Checking Target Variable Distribution**
"""

df['stroke'].value_counts()

sns.set()

"""### 🌟 **Goal**

The command `df['stroke'].value_counts()` is used to **count how many times each unique value** appears in the **stroke column**, which represents our **target variable**.

It helps in identifying whether the dataset is **balanced or imbalanced**, which is crucial for classification tasks.

---

### 🧩 **Explanation**

| Part              | Meaning                                                                |
| ----------------- | ---------------------------------------------------------------------- |
| `df['stroke']`    | Selects the **target column (stroke)** from the DataFrame.             |
| `.value_counts()` | Counts the **frequency of each unique value** (0 or 1) in that column. |

Since the **stroke** column is **binary**:

* **0** → The person **did not** have a stroke.
* **1** → The person **had** a stroke.

# **🧩 Step 14: Visualizing Stroke Case Distribution**
"""

import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(5, 4))
sns.countplot(x='stroke', data=df)
plt.title('Distribution of Stroke Cases')
plt.xlabel('Stroke (0 = No, 1 = Yes)')
plt.ylabel('Count')
plt.show()

sns.countplot(x='hypertension', data=df)

sns.countplot(x='heart_disease', data=df)

sns.countplot(x='ever_married', data=df)

sns.countplot(x='work_type', data=df)

sns.countplot(x='Residence_type', data=df)

sns.countplot(x='smoking_status', data=df)

"""🌼 **Objective**
This code block is used to **visualize the distribution** of the target variable `stroke` with a count plot. It clearly shows how many people **experienced a stroke (1)** versus those who **did not (0)**.

🌈 **Explanation**

| 💬 **Line**                                 | ✨ **Description**                                                                                            |
| ------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| `plt.figure(figsize=(5,4))`                 | Sets the **dimensions** of the plot — width = 5, height = 4.                                                 |
| `sns.countplot(x='stroke', data=df)`        | Creates a **count plot** using Seaborn to show the frequency of each class (0 and 1) in the `stroke` column. |
| `plt.title('Distribution of Stroke Cases')` | Adds a **title** to make the chart more informative.                                                         |
| `plt.xlabel('Stroke (0 = No, 1 = Yes)')`    | Labels the **x-axis**, clarifying the meaning of 0 and 1.                                                    |
| `plt.ylabel('Count')`                       | Labels the **y-axis** to represent the number of records in each group.                                      |
| `plt.show()`                                | **Displays** the final visualization.                                                                        |

🌷 This visualization helps you quickly **compare class distribution** and detect if the data is **imbalanced**, which is vital before training a classification model.

# **📉 Step 22: Visualizing Feature Relationships Using Pairplot**
"""

pair_cols = ['age', 'avg_glucose_level', 'bmi', 'hypertension', 'heart_disease', 'stroke']
# for speed, sample if the dataset is large
sns.pairplot(df[pair_cols].sample(frac=0.25, random_state=42), hue='stroke', corner=True)
plt.suptitle("Pairplot (sampled 25%)", y=1.02)
plt.show()

"""🌻 **Details**

* **pair_cols** → Chooses key numerical and categorical columns for visualization.
* **sample(frac=0.25)** → Takes a 25% random sample of the dataset to make plotting quicker.
* **sns.pairplot()** → Generates scatterplots and histograms for each pair of selected features.
* **hue='stroke'** → Highlights points with different colors based on stroke occurrence.
* **corner=True** → Displays only the lower half of the plot grid to keep it neat and easy to read.

#🪶 **Step 16: Bringing in LabelEncoder and SMOTE**
"""

from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE

"""# 🌈 **Purpose**

These two imports are used for **data preprocessing** and **handling class imbalance** — both are crucial steps before training a machine learning model.

**LabelEncoder** → Converts **categorical data** into **numeric values**.
**SMOTE** → **Balances the dataset** by creating **synthetic samples** for the minority class.

---

# 🔍 **Explanation**

| 🧩 **Library / Function** | 💬 **Description**                                                                                                                                                                                                                                    |
| ------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |




| **LabelEncoder**          | Imported from `sklearn.preprocessing`, it encodes **categorical (text)** data into **numerical form** so that machine learning models can interpret it.                                                                                               |
| **SMOTE**                 | Imported from `imblearn.over_sampling`, it stands for **Synthetic Minority Oversampling Technique** — a method to handle **class imbalance** by generating **synthetic examples** for the minority class instead of merely duplicating existing data. |

# 🌈 Step 15: Encoding Categorical Variables
"""

# Identify categorical columns
cat_cols = ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']

# Initialize label encoder
le = LabelEncoder()

# Apply label encoding to each categorical column
for col in cat_cols:
    df[col] = le.fit_transform(df[col])

"""# **📊 Step: Correlation Heatmap (After Label Encoding)**"""

# Step 1: Encode categorical columns
df_encoded = df.copy()
for col in df_encoded.select_dtypes(include='object').columns:
    df_encoded[col] = df_encoded[col].astype('category').cat.codes

# Step 2: Drop target column and compute correlation
corr = df_encoded.drop('stroke', axis=1).corr()

# Step 3: Plot heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm', square=True)
plt.title("Feature Correlation Matrix (After Label Encoding)")
plt.show()

"""---

🔎 **Objective**  
To explore relationships between numeric and encoded features using correlation analysis.

🧬 **Breakdown**  
`df.drop('stroke', axis=1)` → Excludes the target column to focus only on predictors.  
`.corr()` → Computes correlation scores between all feature pairs.  
`sns.heatmap()` → Visualizes these scores using a color gradient.  
`annot=True` → Displays the exact correlation value inside each grid cell.  
`fmt=".2f"` → Rounds and formats values to two decimal places for clarity.

---

# **✅  Step 17: Splitting Features and Target Variable**
"""

X = df.drop('stroke', axis=1)
y = df['stroke']

"""✅ **[Goal]**
This step divides the dataset into two parts — **independent features (X)** and the **dependent target variable (y)**. It’s an essential step before model training because the model needs to learn the connection between **inputs (X)** and **outputs (y)**.

💡 **[Details]**

| Code                        | Meaning                                                                         |
| --------------------------- | ------------------------------------------------------------------------------- |
| `df.drop('stroke', axis=1)` | Deletes the **‘stroke’** column — the rest become input features (**X**).       |
| `axis=1`                    | Tells Python that we’re removing a **column**, not a row.                       |
| `df['stroke']`              | Chooses the **target column** (‘stroke’) which has 0 (no stroke) or 1 (stroke). |

📊 **[Result]**

* **X** → All input columns (like age, gender, bmi, hypertension, etc.)
* **y** → Target labels (0 or 1)

# **🎯 Step 18: Balancing the Dataset Using SMOTE**
"""

smote = SMOTE(random_state=42)

# Fit and resample
X_resampled, y_resampled = smote.fit_resample(X, y)

"""🌟 **[Objective]**
This step applies **SMOTE (Synthetic Minority Oversampling Technique)** to make the dataset balanced by creating **synthetic samples** for the minority class (stroke = 1).

It helps both classes — **stroke** and **non-stroke** — have equal counts, leading to better **model performance and fairness**.

💫 **[Insight]**

| Code                       | Description                                                                                                                         |
| -------------------------- | ----------------------------------------------------------------------------------------------------------------------------------- |
| `SMOTE(random_state=42)`   | Creates a SMOTE instance. The **random_state=42** keeps results consistent every run.                                               |
| `fit_resample(X, y)`       | Applies SMOTE on the dataset — adds new artificial samples for the minority class.                                                  |
| `X_resampled, y_resampled` | Final balanced data: <br> ➤ **X_resampled** → input features after balancing <br> ➤ **y_resampled** → output labels after balancing |

# **🎯 Step 19: Checking Class Distribution After SMOTE**
"""

from sklearn.preprocessing import LabelEncoder
from imblearn.over_sampling import SMOTE
import pandas as pd

# Step 1: Separate features and target
X = df.drop('stroke', axis=1)
y = df['stroke']

# Step 2: Encode categorical columns in X
le = LabelEncoder()
for col in X.columns:
    if X[col].dtype == 'object':   # if column contains text
        X[col] = le.fit_transform(X[col])

# Step 3: Apply SMOTE
smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)

# Step 4: Convert y_resampled to Pandas Series and check balance
y_resampled = pd.Series(y_resampled, name='stroke')
y_resampled.value_counts()

"""### 🚀 **Objective**

This command is used to **check if the dataset is balanced** after applying **SMOTE (Synthetic Minority Oversampling Technique)**.
It shows how many samples belong to each class — **0 (No Stroke)** and **1 (Stroke)** — in the resampled target variable `y_resampled`.

---

### 🧠 **Description**

| Code Part         | Function                                                         |
| ----------------- | ---------------------------------------------------------------- |
| `y_resampled`     | Represents the **target variable** after SMOTE has been applied. |
| `.value_counts()` | Displays the **number of occurrences** for each class (0 and 1). |

By using this, we can **verify** that SMOTE has successfully **created equal samples** for both classes, ensuring the dataset is properly balanced.

#🚀 **Stage 20: Loading train_test_split**
"""

from sklearn.model_selection import train_test_split

"""# **🌟 Objective**

The `train_test_split` function helps divide the dataset into two key sections:

* **Training set** → used to teach the machine learning model.
* **Testing set** → used to check how accurately the model performs on new, unseen data.

This step makes sure the model **learns patterns** rather than just **memorizing data**, ensuring better generalization.

---

# **💡 Insight**

| Element                   | Meaning                                                                                                  |
| ------------------------- | -------------------------------------------------------------------------------------------------------- |
| `sklearn.model_selection` | A part of the Scikit-learn library that offers tools for splitting data and assessing model performance. |
| `train_test_split`        | A method from this module that divides data randomly into training and testing sets.                     |

# **🎯Step 21: Splitting the Dataset into Training and Testing Sets**
"""

X_train, X_test, y_train, y_test = train_test_split(
    X_resampled,
    y_resampled,
    test_size=0.2,
    random_state=42,
    stratify=y_resampled)

"""#✨ **[Aim]**

The purpose of this step is to divide the balanced dataset into **training** and **testing** parts.

This helps us to:

* 🧩 **Train** the model using one portion (training set).
* 🔍 **Evaluate** its accuracy on unseen data (testing set).

By doing this, we make sure the model **learns patterns** that can be applied generally — not just memorized data.

---

💫 **[Details of Each Parameter]**

| Parameter                | Meaning                                                                                                      |
| ------------------------ | ------------------------------------------------------------------------------------------------------------ |
| **X_resampled**          | The input features obtained after applying SMOTE.                                                            |
| **y_resampled**          | The balanced target variable (0 = no stroke, 1 = stroke).                                                    |
| **test_size=0.2**        | Reserves 20% of the data for testing and 80% for training.                                                   |
| **random_state=42**      | Guarantees consistent results whenever the code is executed.                                                 |
| **stratify=y_resampled** | Maintains equal class distribution in both training and testing sets — preventing imbalance after the split. |

# **🎯Step 22: Checking the Shape of Training and Testing Sets**
"""

print("Training Set Shape:", X_train.shape)
print("Testing Set Shape:", X_test.shape)

"""### 🌈 **Aim**

This step is performed to **confirm the shape and organization** of the training and testing datasets after the data has been divided.

It helps ensure that the **split was done correctly** and that the number of samples in each portion aligns with the chosen ratio (for example, 80% training and 20% testing).

---

### 🧩 **Explanation**

| Code Part       | Function                                                                                                           |
| --------------- | ------------------------------------------------------------------------------------------------------------------ |
| `X_train.shape` | Shows a pair of values indicating the **number of rows (records)** and **columns (features)** in the training set. |
| `X_test.shape`  | Displays the same details for the testing dataset.                                                                 |
| `print()`       | Outputs both shapes neatly so that they can be easily checked and compared.                                        |

# **🎯Step 16: Importing Multiple Machine Learning Models and Evaluation Metrics**
"""

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

"""💎 **[Goal]**
This section brings in several **classification algorithms** and **evaluation metrics** from the **Scikit-learn** library.

It helps us **train, test, and compare** multiple models to discover which one performs best for **stroke prediction**.

---

🧩 **[Overview of Algorithms]**

🔸 **LogisticRegression**
📍 *From:* `sklearn.linear_model`
🎯 *Function:* Performs binary classification using a straight-line boundary.
💼 *When to Use:* Works best when there’s a linear relationship between features and target.
💪 *Strong Point:* Simple, quick, and easy to interpret through coefficients.

---

🌳 **DecisionTreeClassifier**
📍 *From:* `sklearn.tree`
🎯 *Function:* Builds a flowchart-like structure of decisions and results.
💼 *When to Use:* Handles non-linear data efficiently.
💪 *Strong Point:* Easy to understand and visualize.
⚠️ *Weak Point:* Can overfit if not pruned correctly.

---

🌲 **RandomForestClassifier**
📍 *From:* `sklearn.ensemble`
🎯 *Function:* Combines several decision trees to improve accuracy.
💼 *When to Use:* Best for minimizing overfitting and improving reliability.
💪 *Strong Point:* High accuracy and resistance to noisy data.
⚠️ *Weak Point:* Harder to interpret than a single decision tree.



---

🔥 **GradientBoostingClassifier**
📍 *From:* `sklearn.ensemble`
🎯 *Function:* Builds trees one after another — each fixing errors from the last.
💼 *When to Use:* Excellent for complex and imbalanced datasets.
💪 *Strong Point:* High prediction accuracy and efficiency.
⚠️ *Weak Point:* Training can be slower and needs careful tuning.



---

⚙️ **SVC (Support Vector Classifier)**
📍 *From:* `sklearn.svm`
🎯 *Function:* Finds the best boundary that separates two classes with the widest margin.
💼 *When to Use:* Works great for both linear and non-linear cases (via kernels).
💪 *Strong Point:* Handles high-dimensional data effectively.
⚠️ *Weak Point:* Slow on large datasets and sensitive to scaling.






---

👥 **KNeighborsClassifier (KNN)**
📍 *From:* `sklearn.neighbors`
🎯 *Function:* Assigns class based on the majority of nearest neighbors.
💼 *When to Use:* Perfect for small datasets and pattern recognition.
💪 *Strong Point:* No training phase needed.
⚠️ *Weak Point:* Predictions can be slow and affected by irrelevant features.



---

📏 **Evaluation Metrics**

| Metric                       | Purpose                                                          |
| ---------------------------- | ---------------------------------------------------------------- |
| 🧮 **accuracy_score**        | Measures how often the model’s predictions are correct.          |
| 🧾 **confusion_matrix**      | Displays true vs predicted results to see where mistakes happen. |
| 📋 **classification_report** | Gives detailed stats like precision, recall, and F1-score.       |

---







📊 **[Why Try Multiple Models]**

* To compare how different algorithms perform on the same dataset.
* To pick the best model using metrics like **accuracy**, **recall**, and **F1-score**.
* Each algorithm learns patterns differently — testing several ensures **robustness and reliability**.

---




🏁 **[Wrap-Up]**
This step lays the **foundation for fair model comparison and evaluation**.
By importing and using various classifiers and metrics, we can confidently choose the model — whether **simple (Logistic Regression)** or **advanced (Random Forest, Gradient Boosting)** — that provides the **most dependable stroke predictions**.

# **🧠  Step 17: Training and Evaluating Multiple Machine Learning Models**
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix

# Train the best model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)

# Predictions
y_pred = rf_model.predict(X_test)

# Evaluation
accuracy = accuracy_score(y_test, y_pred)
print(f"Random Forest Accuracy: {accuracy:.4f}")

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

print("\nConfusion Matrix:")
print(confusion_matrix(y_test, y_pred))

"""---

## ⚙️ **Training and Comparing Multiple Machine Learning Models**

🎯 **Purpose**
This code trains several machine learning models on the training data and evaluates their accuracy on the test data.
It helps determine **which model gives the best prediction performance** for stroke detection.

---

🧠 **Explanation**

### 🔹 Defining the Models

```python
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "Gradient Boosting": GradientBoostingClassifier(),
    "SVM": SVC(),
    "KNN": KNeighborsClassifier()
}
```

A dictionary named **`models`** is created.
Each **key** represents the model’s name (as text), and each **value** is the actual model object from **scikit-learn**.
This structure allows testing multiple models easily in a single loop — no need to write separate code for each.

---

### 🔹 Creating a Results Dictionary

```python
results = {}
```

An empty dictionary **`results`** is made to store the accuracy score of each model.
Example:
`{'Logistic Regression': 0.85, 'Random Forest': 0.92, ...}`

---

### 🔹 Training and Testing Each Model

**Step-by-step:**

1. **Loop through models:**

   ```python
   for name, model in models.items():
   ```

   Goes through every model and its name in the dictionary.

2. **Train the model:**

   ```python
   model.fit(X_train, y_train)
   ```

   Fits (trains) the model using the training data.

3. **Make predictions:**

   ```python
   y_pred = model.predict(X_test)
   ```

   Uses the trained model to predict outcomes on test data.

4. **Measure accuracy:**

   ```python
   acc = accuracy_score(y_test, y_pred)
   ```

   Compares predicted labels (`y_pred`) with actual labels (`y_test`) to calculate accuracy.

5. **Store results:**

   ```python
   results[name] = acc
   ```

   Saves each model’s accuracy inside the `results` dictionary.

6. **Display accuracy:**

   ```python
   print(f"{name} Accuracy: {acc:.4f}")
   ```

   Prints every model’s accuracy score (rounded to 4 decimal places).

---

# ⚙️ **Step 18: Displaying Model Accuracy Scores**

---

# 🌟 **⚙️ Displaying Model Accuracy Scores**

---

### 🎯 **Purpose**

To **print and compare** how accurately each **machine learning model** predicts the target variable (**stroke**).
This step helps in **quickly identifying** which algorithm performs the **best** among all tested models.

---

### 🧠 **Explanation**

#### 🔹 **1️⃣ Loop Through Models**

```python
for name, model in models.items():
```

Iterates through every model and its name stored in the dictionary.

---

#### 🔹 **2️⃣ Evaluate Accuracy**

```python
model.score(X_test, y_test)
```

Calculates how well the model performs on unseen test data.
Returns a value between **0 and 1** — for example, `0.91 → 91% accuracy`.

---

#### 🔹 **3️⃣ Format Accuracy as Percentage**

```python
"{:.2f}%".format(model.score(X_test, y_test) * 100)
```

Converts the accuracy score into **percentage form** and displays it
up to **two decimal places** for better readability.

---

#### 🔹 **4️⃣ Print Model Name with Accuracy**

```python
print(name + ": {:.2f}%".format(model.score(X_test, y_test) * 100))
```

Displays each model’s name along with its **accuracy percentage** in a clear format.

---

### 🖨️ **Example Output**

```
Logistic Regression: 84.20%
Decision Tree: 90.10%
Random Forest: 92.45%
Gradient Boosting: 93.00%
SVM: 87.60%
KNN: 88.10%
```

---

### 💡 **In Short**

This step helps in **training, testing, and comparing** multiple ML models
to choose the one that provides the **highest accuracy** for predicting **stroke outcomes** efficiently.

---

# **📊 Step 19: Visualizing Confusion Matrices for All Models**
"""

from sklearn.metrics import confusion_matrix
import seaborn as sns
import matplotlib.pyplot as plt

# Predict using the best model
y_pred = rf_model.predict(X_test)

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
print("Confusion Matrix:")
print(cm)

# Plot the confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={'size': 14})
plt.xlabel('Predicted Labels', fontsize=14)
plt.ylabel('True Labels', fontsize=14)
plt.title('Confusion Matrix - Random Forest', fontsize=16)
plt.show()

"""💫 **[Aim]**
This section evaluates how well each model performs by generating a **Confusion Matrix** and turning it into a **heatmap visualization**.

It clearly highlights where the model makes **accurate predictions** and where it **misclassifies** the data, giving a crystal-clear performance overview.

---

🧠 **[Deep Dive Explanation]**

### 🔸 Looping Through Each Model

```python
for name, model in models.items():
```

* Iterates through every trained model stored inside the `models` dictionary.
* **name** → name of the model (e.g., *“Random Forest”*)
* **model** → actual trained model ready for evaluation

---

### 🔸 Making Predictions

```python
y_pred = model.predict(X_test)
```

* Uses the trained model to predict outcomes for `X_test`.
* The predictions are stored in `y_pred` to compare later with the actual results (`y_test`).

---

### 🔸 Creating the Confusion Matrix

```python
cm = confusion_matrix(y_test, y_pred)
```

* Builds a 2×2 matrix (for binary classification) showing:

  * ✅ **True Positives (TP):** Correctly identified stroke cases
  * ✅ **True Negatives (TN):** Correctly identified non-stroke cases
  * ⚠️ **False Positives (FP):** Predicted stroke when it wasn’t
  * ⚠️ **False Negatives (FN):** Missed predicting an actual stroke

📊 **Example Output:**

```
Confusion Matrix
[[950  10]
 [ 35  15]]
```

---

### 🔸 Visualizing the Results with Heatmap

```python
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False, annot_kws={'size':14})
```

✨ Creates a **color-coded visualization** of the confusion matrix:

* `annot=True` → Displays numbers inside each cell
* `fmt='d'` → Formats as whole numbers
* `cmap='Blues'` → Uses soft blue shades
* `cbar=False` → Removes color bar for clean look
* `annot_kws={'size':14}` → Makes text inside cells more readable

💡 This helps you **visually compare** correct vs. incorrect predictions instantly.

---

# 🔸 Adding Labels and Title

```python
plt.xlabel('Predicted Labels', fontsize=14)
plt.ylabel('True Labels', fontsize=14)
plt.title(f'Confusion Matrix - {name}', fontsize=16)
```

* Adds clear labels for both axes
* Shows which labels were predicted vs. which were true
* Displays the title for each model dynamically (e.g., *“Confusion Matrix - Random Forest”*)

---

### 🔸 Displaying the Final Plot

```python
plt.show()
```

🎨 Shows the heatmap for each model one by one — making it easy to visually compare performance.

---

#📈 **Example Visualization Meaning:**

|                | Predicted No | Predicted Yes |
| -------------- | ------------ | ------------- |
| **Actual No**  | TN = 950     | FP = 10       |
| **Actual Yes** | FN = 35      | TP = 15       |

🔵 **Darker blue** → Correct predictions
🔷 **Lighter shades** → Misclassifications

---

✨ **In Short:**
This phase helps you **analyze each model’s prediction behavior** visually and statistically — ensuring you don’t just rely on accuracy numbers but also **understand exactly where and how each model makes mistakes.**

# **📋 Step 20: Generating Classification Reports for Each Model**

````markdown
# 🧾 **Step 20: Evaluating Model Performance with Classification Reports**

🌟 **Purpose**

This step generates a **comprehensive performance summary** for every trained model using the `classification_report()` function.  
It highlights key metrics — **Precision**, **Recall**, **F1-Score**, and **Accuracy**, helping us understand how effectively each model predicts stroke outcomes.

---

🧠 **Explanation**

🔸 **Iterating Through All Models**  
```python
for name, model in models.items():
````

Goes through each trained model stored in the `models` dictionary.

* **name** → represents the model’s title (e.g., “Random Forest”, “SVM”).
* **model** → refers to the trained classifier object.

---

🔸 **Generating Predictions**

```python
y_pred = model.predict(X_test)
```

Each model predicts output labels (`0` or `1`) for the test dataset `X_test`.
These predictions are stored in `y_pred` to compare with the actual results (`y_test`).

---

🔸 **Displaying the Model Name**

```python
print(name + ":")
```

Prints the current model’s name to make the report output structured and easy to follow.

---

🔸 **Creating the Classification Report**

```python
report = classification_report(y_test, y_pred)
```

This function from **scikit-learn** generates an evaluation summary showing:

| Metric                   | Description                                                                 |
| :----------------------- | :-------------------------------------------------------------------------- |
| **Precision**            | Out of all predicted positives, how many were correct? *(TP / (TP + FP))*   |
| **Recall (Sensitivity)** | Out of all actual positives, how many did we detect? *(TP / (TP + FN))*     |
| **F1-Score**             | The balanced mean of Precision and Recall.                                  |
| **Support**              | Total count of true instances for each class *(0 = No Stroke, 1 = Stroke)*. |

---

🔸 **Printing the Report**

```python
print(report)
```

Displays a detailed, tabular performance summary for each model.

---

📊 **Example Output:**

```
Random Forest:
              precision    recall  f1-score   support
       0          0.97      0.99      0.98       965
       1          0.85      0.72      0.78        55

accuracy                            0.97      1020
macro avg       0.91      0.86      0.88      1020
weighted avg    0.97      0.97      0.97      1020
```

---

💡 **In Short:**
This step helps you **analyze and compare the true predictive performance** of all models — allowing you to confidently choose the most accurate and reliable one for detecting stroke risk.

```


```

# **📈 Step 21: Comparing Model Performance Using Evaluation

   #                         Metrics and Visualization**
"""

# 📦 Import libraries
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression, RidgeClassifier
from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier, BaggingClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from imblearn.over_sampling import SMOTE
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# 🧪 Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ⚖️ Scale features
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 🔁 Apply SMOTE to training data
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train_scaled, y_train)

# 🤖 Define multiple classifiers
models = {
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Ridge Classifier': RidgeClassifier(),
    'Decision Tree': DecisionTreeClassifier(),
    'Extra Tree': ExtraTreeClassifier(),
    'Random Forest': RandomForestClassifier(),
    'Gradient Boosting': GradientBoostingClassifier(),
    'AdaBoost': AdaBoostClassifier(),
    'Bagging': BaggingClassifier(),
    'SVM': SVC(),
    'KNN': KNeighborsClassifier(),
    'Naive Bayes': GaussianNB()
}

# 📊 Evaluate models
model_results = []

for name, model in models.items():
    model.fit(X_train_resampled, y_train_resampled)
    y_pred = model.predict(X_test_scaled)

    acc = accuracy_score(y_test, y_pred)
    prec = precision_score(y_test, y_pred, zero_division=0)
    rec = recall_score(y_test, y_pred, zero_division=0)
    f1 = f1_score(y_test, y_pred, zero_division=0)

    model_results.append([name, acc, prec, rec, f1])

# 📋 Create results DataFrame
results_df = pd.DataFrame(model_results, columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score'])

# 📢 Print results
print("\n📊 Model Performance Comparison:")
print(results_df)

# 🎨 Visualize results
plt.figure(figsize=(12,7))
sns.barplot(data=results_df.melt(id_vars='Model', var_name='Metric', value_name='Score'),
            x='Score', y='Model', hue='Metric', palette='viridis')
plt.title('Model Performance Comparison', fontsize=16)
plt.xlabel('Score')
plt.ylabel('Model')
plt.legend(loc='lower right')
plt.tight_layout()
plt.show()

"""---

# **🚀 Objective**

In this stage, we analyze how each trained model performs using various evaluation measures — **Accuracy**, **Precision**, **Recall**, and **F1-Score** — and then present their comparison through an eye-catching **bar chart**.

This process helps determine **which ML algorithm delivers the best results** for predicting stroke outcomes.

---

# **💡 Detailed Breakdown**

### 🔸 **Importing Evaluation Metrics**

These functions help us measure how well the models perform in classification:

* **Accuracy** → Proportion of total correct predictions.
* **Precision** → Out of all predicted “stroke” cases, how many were truly strokes.
* **Recall** → Out of all real stroke cases, how many the model correctly identified.
* **F1-Score** → A combined measure balancing both precision and recall.

---

### 🔸 **Initializing the Results List**

`model_results = []`
Starts an empty list to hold performance scores of each trained model.

Each record will later store values in this format:
➡️ `[Model Name, Accuracy, Precision, Recall, F1 Score]`

---

### 🔸 **Looping Through Each Trained Model**

Every model from the `models` dictionary is tested here.

* The model predicts on **X_test** data.
* The predictions are saved as **y_pred**.
* These predictions are then evaluated through performance metrics.

---

### 🔸 **Calculating Model Metrics**

* `accuracy_score` → Checks overall correctness.
* `precision_score` → Tests how precise the stroke identification was.
* `recall_score` → Finds how many real stroke cases were captured.
* `f1_score` → Blends precision and recall for a balanced view.

---

### 🔸 **Storing the Evaluation Results**

Each model’s calculated metrics are added as a new record inside the list.
Example layout:

```
[
  ['Logistic Regression', 0.84, 0.72, 0.68, 0.70],
  ['Random Forest', 0.92, 0.89, 0.80, 0.84]
]
```

---

### 🔸 **Creating a Structured DataFrame**

The results are transformed into a **pandas DataFrame** with clear columns:
**Model**, **Accuracy**, **Precision**, **Recall**, **F1 Score**.

This structure makes the results more readable and ready for visualization.

---

### 🔸 **Printing the Comparison Table**

```python
print("\n📊 Model Performance Comparison:")
print(results_df)
```

Shows a clear table comparing all trained models and their key performance scores.

Example Output:

| Model               | Accuracy | Precision | Recall | F1 Score |
| ------------------- | -------- | --------- | ------ | -------- |
| Logistic Regression | 0.84     | 0.72      | 0.68   | 0.70     |
| Random Forest       | 0.92     | 0.89      | 0.80   | 0.84     |

---

### 🔸 **Visualizing the Comparison**

* **`results_df.melt()`** → Reshapes the table for Seaborn plotting.
* **`sns.barplot()`** → Draws a grouped bar chart showing all metrics by model.
* **`palette='viridis'`** → Gives a pleasant gradient color scheme.

📈 Titles and axis labels make the chart clear and professional for interpretation.

---



---

# 🧠 **Summarised Section**

---

# 🧠 Stroke Prediction using Machine Learning — Project Summary

## 📋 Overview

This project aims to predict the probability of a stroke in patients by analyzing medical, lifestyle, and demographic information.
Machine learning algorithms are utilized to uncover key risk indicators and develop predictive models that can aid in early diagnosis and prevention.

The dataset consists of 5,110 patient records with 12 key features such as **age, gender, hypertension, heart disease, BMI, glucose levels,** and **smoking status.**
The target variable **stroke** specifies whether a patient experienced a stroke (`1`) or not (`0`).

---

## 🧩 Methodology

### 1️⃣ Data Preprocessing

* Loaded the dataset using **Pandas** and explored its structure.
* Filled missing values in **BMI** using the mean value.
* Checked and removed duplicate entries.
* Encoded categorical data using **LabelEncoder**.
* Applied **SMOTE (Synthetic Minority Oversampling Technique)** to balance the dataset since stroke cases were underrepresented.

### 2️⃣ Exploratory Data Analysis (EDA)

* Used **Seaborn** and **Matplotlib** for visualizations.
* Created **correlation heatmaps** to identify relationships among variables.
* Found that **age**, **hypertension**, and **glucose level** are strongly linked to stroke risk.

### 3️⃣ Model Development

Trained and evaluated six different machine learning algorithms:

* Logistic Regression
* Decision Tree
* Random Forest 🌟 (Best Performer)
* Gradient Boosting
* Support Vector Machine (SVM)
* K-Nearest Neighbors (KNN)

Data was divided into **80% training** and **20% testing** sets using **train_test_split** with stratification to maintain class balance.

### 4️⃣ Model Evaluation

Each model was assessed using the following performance metrics:

* **Accuracy**
* **Precision**
* **Recall**
* **F1-Score**
* **Confusion Matrix**

---

## 🏆 Results

| Model               | Accuracy   |
| :------------------ | :--------- |
| Logistic Regression | 0.7938     |
| Decision Tree       | 0.9044     |
| **Random Forest** ✅ | **0.9424** |
| Gradient Boosting   | 0.8751     |
| SVM                 | 0.5111     |
| KNN                 | 0.8077     |

The **Random Forest Classifier** delivered the best results with an accuracy of **94.24%**, showing strong balance between precision and recall in stroke prediction.

---

## 📈 Key Findings

* **Age**, **hypertension**, and **glucose level** emerged as the most critical factors.
* Applying **SMOTE** significantly enhanced prediction fairness and stability.
* **Ensemble models** (Random Forest, Gradient Boosting) achieved higher accuracy and generalization than single algorithms.

---

## 💡 Conclusion

This project proves how **machine learning** can play a vital role in predicting stroke risks by evaluating multiple health indicators.
The final **Random Forest model** serves as a robust foundation for early detection and preventive health analytics.

---

## 🌠 Next Possibilities

* Fine-tune model parameters using **GridSearchCV** for greater optimization.
* Transform the model into a **Streamlit or Flask web app** for live stroke prediction.
* Integrate **more clinical data** (e.g., cholesterol, blood pressure) to boost predictive precision.
* Add **explainable AI tools** like **SHAP** or **LIME** to make predictions more interpretable.

---
"""